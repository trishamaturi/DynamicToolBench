{
    "product_id": "api_b0089109-ef22-4854-9048-ef0c6a33ec65",
    "tool_description": "The Crawling API allows for fast and efficient web crawling and scraping while staying anonymous. This API can be easily integrated with your favorite language or framework.",
    "home_url": "https://rapidapi.com/proxycrawl/api/proxycrawl-crawling/",
    "name": "ProxyCrawl Crawling",
    "title": "ProxyCrawl Crawling",
    "pricing": "FREEMIUM",
    "tool_name": "ProxyCrawl Crawling",
    "score": {
        "avgServiceLevel": 99,
        "avgLatency": 5969,
        "avgSuccessRate": 99,
        "popularityScore": 9.6,
        "__typename": "Score"
    },
    "host": "proxycrawl-crawling.p.rapidapi.com",
    "api_list": [
        {
            "name": "/_copy",
            "url": "https://proxycrawl-crawling.p.rapidapi.com/",
            "description": "Crawls and Scrapes the Web with a given URL.\n\nExample: GET / ?url=https://www.amazon.com",
            "method": "GET",
            "required_parameters": [
                {
                    "name": "url",
                    "type": "STRING",
                    "description": "A url to crawl. Make sure it starts with http or https and that is fully encoded.",
                    "default": "https://www.amazon.com"
                }
            ],
            "optional_parameters": [
                {
                    "name": "country",
                    "type": "STRING",
                    "description": "If you want your requests to be geolocated from a specific country, you can use the &country= parameter, like &country=US (two-character country code).\n\nPlease take into account that specifying a country can reduce the number of successful requests you get back, so use it wisely and only when geolocation crawls are required.\n\nAlso note that some websites like Amazon are routed via different special proxies and all countries are allowed regardless of being in the list or not.",
                    "default": ""
                },
                {
                    "name": "cookies_session",
                    "type": "STRING",
                    "description": "If you need to send the cookies that come back on every request to all subsequent calls, you can use the &cookies_session= parameter.\n\nThe &cookies_session= parameter can be any value. Simply send a new value to create a new cookies session (this will allow you to send the returned cookies from the subsequent calls to the next API calls with that cookies session value). Sessions expire in 300 seconds after the last API call.",
                    "default": ""
                },
                {
                    "name": "request_headers",
                    "type": "STRING",
                    "description": "If you need to send request headers to the original website, you can use the &request_headers=EncodedRequestHeaders parameter.\n\nExample request headers: accept-language:en-GB|host:api.proxycrawl.com\n\nExample encoded: &request_headers=accept-language%3Aen-GB%7Chost%3Aapi.proxycrawl.com\n\nPlease note that not all request headers are allowed by the API. We recommend that you test the headers sent using this testing url: https://httpbin.org/headers\n\nIf you need to send some additional headers which are not allowed by the API, please let us know the header names and we will authorize them for your token.",
                    "default": ""
                },
                {
                    "name": "store",
                    "type": "STRING",
                    "description": "Stores a copy of the API response in the [ProxyCrawl Cloud Storage](https://proxycrawl.com/dashboard/storage)",
                    "default": ""
                },
                {
                    "name": "scraper",
                    "type": "STRING",
                    "description": "Returns back the information parsed according to the specified scraper. Check the [list of all the available data scrapers](https://proxycrawl.com/docs/crawling-api/scrapers/) (opens new window)list of all the available data scrapers] to see which one to choose.\n\nThe response will come back as JSON.",
                    "default": ""
                },
                {
                    "name": "proxy_session",
                    "type": "STRING",
                    "description": "If you need to use the same proxy for subsequent requests, you can use the &proxy_session= parameter.\n\nThe &proxy_session= parameter can be any value. Simply send a new value to create a new proxy session (this will allow you to continue using the same proxy for all subsequent requests with that proxy session value). Sessions expire 30 seconds after the last API call.\n\n",
                    "default": ""
                },
                {
                    "name": "autoparse",
                    "type": "BOOLEAN",
                    "description": "If you need to get the scraped data of the page that you requested, you can pass &autoparse=true parameter.\n\nThe response will come back as JSON. The structure of the response varies depending on the URL that you sent.\n\nPlease note: &autoparse=true is an optional parameter. If you don't use it, you will receive back the full HTML of the page so you can scrape it freely.",
                    "default": ""
                },
                {
                    "name": "get_headers",
                    "type": "BOOLEAN",
                    "description": "If you need to get the headers that the original website sets on the response, you can use the &get_headers=true parameter.\n\nThe headers will come back in the header (or in the json response if you use &format=json) as original_header_name.",
                    "default": ""
                },
                {
                    "name": "get_cookies",
                    "type": "BOOLEAN",
                    "description": "If you need to get the cookies that the original website sets on the response, you can use the &get_cookies=true parameter.",
                    "default": ""
                },
                {
                    "name": "tor_network",
                    "type": "BOOLEAN",
                    "description": "If you want to crawl onion websites over the Tor network, you can pass the &tor_network=true parameter.",
                    "default": ""
                },
                {
                    "name": "cookies",
                    "type": "STRING",
                    "description": "If you need to send cookies to the original website, you can use the &cookies=EncodedCookies parameter.\n\nExample cookies: key1=value1; key2=value2; key3=value3\n\nExample encoded: &cookies=key1%3Dvalue1%3B%20key2%3Dvalue2%3B%20key3%3Dvalue3\n\nWe recommend that you test the cookies sent using this testing url: https://httpbin.org/cookies",
                    "default": ""
                },
                {
                    "name": "device",
                    "type": "STRING",
                    "description": "If you don't want to specify a user_agent but you want to have the requests from a specific device, you can use this parameter.\n\nThere are two options available: desktop and mobile.",
                    "default": ""
                },
                {
                    "name": "format",
                    "type": "STRING",
                    "description": "Indicates the response format, either json or html. Defaults to html.\n\nIf format html is used, ProxyCrawl will send you back the response parameters in the headers (see [HTML response](https://proxycrawl.com/docs/crawling-api/response/#html-response)).",
                    "default": ""
                },
                {
                    "name": "user_agent",
                    "type": "STRING",
                    "description": "Make the request with a custom user agent.",
                    "default": ""
                }
            ],
            "code": "import requests\n\nurl = \"https://proxycrawl-crawling.p.rapidapi.com/\"\nquerystring = {\"country\": \"\", \"cookies_session\": \"\", \"request_headers\": \"\", \"store\": \"\", \"scraper\": \"\", \"proxy_session\": \"\", \"autoparse\": \"\", \"get_headers\": \"\", \"get_cookies\": \"\", \"tor_network\": \"\", \"cookies\": \"\", \"url\": \"https://www.amazon.com\", \"device\": \"\", \"format\": \"\", \"user_agent\": \"\"}\n\nheaders = {\n            \"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\",\n            \"X-RapidAPI-Host\": \"proxycrawl-crawling.p.rapidapi.com\"\n        }\n\nresponse = requests.get(url, headers=headers, params=querystring)\nprint(response.json())\n",
            "convert_code": "import requests\n\nurl = \"https://proxycrawl-crawling.p.rapidapi.com/\"\nquerystring = {\"country\": \"\", \"cookies_session\": \"\", \"request_headers\": \"\", \"store\": \"\", \"scraper\": \"\", \"proxy_session\": \"\", \"autoparse\": \"\", \"get_headers\": \"\", \"get_cookies\": \"\", \"tor_network\": \"\", \"cookies\": \"\", \"url\": \"https://www.amazon.com\", \"device\": \"\", \"format\": \"\", \"user_agent\": \"\"}\n\nheaders = {\n            \"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\",\n            \"X-RapidAPI-Host\": \"proxycrawl-crawling.p.rapidapi.com\"\n        }\n\nresponse = requests.get(url, headers=headers, params=querystring)\nprint(response.json())\n",
            "test_endpoint": ""
        }
    ]
}